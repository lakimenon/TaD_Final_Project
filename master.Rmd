---
title: "COVID_TopicModel"
author: "Angela Teng"
date: "4/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Text as Data Final Project 
Lakshmi Menon, Angela Marie Teng
```{r echo=TRUE}

# get current working directory
rm(list = ls())
getwd()  
# setwd("~/Data/articles")

```
## Loading Libraries
```{r include = FALSE}


# loading packages 
library(dplyr)
library(ggplot2)
library(xtable)
library(plyr)

library(rvest)
library(dplyr)
# install.packages("proxy")
library(proxy)
# Loading multiple packages
libraries <- c("foreign", "stargazer")
lapply(libraries, require, character.only=TRUE)
set.seed(100)
library(quanteda)
library("devtools")
# install.packages("textreadr")
library(textreadr)

library(rjson)

# Load it into our environment
library(quanteda.corpora)
library(quanteda)
# install.packages("tm")
library(tm)
library(readtext)
# install.packages("spacyr")
library(spacyr)

libraries <- c("topicmodels", "dplyr", "stm", "quanteda")
lapply(libraries, require, character.only = T)
# to check version
packageVersion("quanteda")
# install.packages("magicfor")
library(magicfor)


library(Rtsne)
library(rsvd)
library(geometry)


library(gridExtra)

library(bursts)

library(readtext)

library(lsa)
```

Amber Notes: 
- So I think we can create and use different DFMs with different pre-processing methods and compare that instead of using just 1 dfm. It would be cool to see how preprocessing affects our results 

**DFMS:**
dfm <- dfm of entire corpus; dfm(covid_corp, tolower=TRUE, stem = TRUE,  remove = stopwords("english"), remove_punct = TRUE)
dfm_trimmed <- dfm_trim(dfm, min_termfreq=20, min_docfreq=2)


dfm_2 <- removing numbers from here 

Actual DFM used for everything: 
dfm_2 <- dfm(covid_corp, tolower=TRUE, stem = TRUE,  remove = c(stopwords("english"), tad_stopwords), remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_hyphens = TRUE, remove_symbols = TRUE)
dfm_2

## Preprocessing 

Loading in all the articles from rev.com:
```{r data_load, echo =TRUE}
# temp = list.files(pattern="*.txt")
# myfiles = lapply(temp, read.delim)

## the read in of a directory
data = read_dir('/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/articles')

summary(data)
typeof(data)
attributes(data)
```

Selecting only articles that contain keywords per country: 
UK - UK, [Boris] Johnson
US - US, Trump 
New Zealand - New Zealand, [Jacinda] Ardern
Australia - Australia, [Scott] Morrison
Canada - Canada, [Justin] Trudeau

(this data is from Data/Articles folder)

Load in scraped data: 

```{r data_scraped, echo = TRUE}
load('./Workspaces/scraped_dates.RData')
summary(matches)

matches$country <- ifelse(grepl("Donald|Trump|US|United States", matches$title), "US", "country")

matches$country <- ifelse(grepl("Donald|Trump|US|United States", matches$title), "US", matches$country)

matches$country <- ifelse(grepl("Boris|Johnson|United Kingdom|UK", matches$title), "UK", matches$country)

matches$country <- ifelse(grepl("Jacinda|Ardern|New Zealand", matches$title), "NZ", matches$country)

matches$country <- ifelse(grepl("Scott|Morrison|Australia", matches$title), "AU", matches$country)

matches$country <- ifelse(grepl("Justin|Trudeau|Canada", matches$title), "CA", matches$country)

head(matches)

colnames(matches)
typeof(matches)
attributes(matches)

# write.csv(matches,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/matches.csv")


```

```{r subset_data, echo=TRUE}
matches[,grepl("trump", colnames("title"))]

# turn list into df
df <- data.frame(matches)
# sanity check
df[1,1:2]

```

Now, we want to preprocess our text and convert it into a dfm
```{r preprocessing, echo = TRUE}

# turn list into df
df <- as.data.frame(matches, stringsAsFactors=F)

attributes(df)

typeof(df)
typeof(df$text)

mode(df$text)
storage.mode(df$text) <- "character"

covid_corp <- corpus(df, text_field ="text" )

df <- apply(df,2,as.character)
head(df)

write.csv(df,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/master.csv")


head(covid_corp)
# a corpus consists ofs: (1) documents: text + doc level data (2) corpus metadata (3) extras (settings)
head(docvars(covid_corp))  # document-level variables
metacorpus(covid_corp)  # corpus-level variables

# ndoc identifies the number of documents in a corpus
ndocs <- ndoc(covid_corp)
# this is because it's subdividedd into paragraph, rather than article.....
ndocs

corpusinfo <- summary(covid_corp, n = ndocs)  # note n default is 100
head(corpusinfo)
```



STM recommended preprocessing from paper (this takes super long to run)
```{r echo = TRUE}
# STM Recommended preprocessing from paper

# data <- read.csv("./Data/master.csv")
# head(data)
length(covid_corp$documents)

# note: this code is very slow
# apply STM Vignette recommended preprocessing
# processed <- textProcessor(covid_corp$documents)

# save.image("./Workspaces/STM_preprocessing.RData")
# load("./Workspaces/STM_preprocessing.RData")
# out<- prepDocuments(processed$documents, processed$vocab, processed$meta)
# docs <- out$documents
# vocab <- out$vocab

# @Amber question: Should we remove numbers in our preprocessing?

```

## DFM Creation 

Quanteda Preprocessing and DFM creation
```{r echo = TRUE}

# Using data from the corpus
tokens <- tokens(covid_corp, remove_punct = TRUE, remove_numbers = FALSE, remove_symbols = TRUE, remove_url = TRUE,  remove_hyphens=TRUE) 

# tokens
num_tokens <- sum(lengths(tokens))
num_tokens

dfm <- dfm(covid_corp, tolower=TRUE, stem = TRUE,  remove = stopwords("english"), remove_punct = TRUE)
dfm

# remove speaker from stopwords

tad_stopwords <- c("speaker","inaudible", "foreign language")

?textProcessor

# ACTUAL DFM USED FOR EVERYTHING
dfm_2 <- dfm(covid_corp, tolower=TRUE, stem = TRUE,  remove = c(stopwords("english"), tad_stopwords), remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_hyphens = TRUE, remove_symbols = TRUE)
dfm_2

save.image("./Workspaces/dfm_2.RData")

```

Basic EDA by country - the countries that say country aren't associated with our keywords (do not have our keywords in the title)
```{r dfm, echo=TRUE}
head(docvars(covid_corp))

# Create a table that shows how many documents are associated with each newspaper
# this works for R 3.5 but not 3.6 
associated_docs <- aggregate(texts ~ country, data=as.data.frame(covid_corp$documents), FUN=length)
associated_docs

table(covid_corp$documents$country)
# country is probably NA (meaning the title didn't match any of the key words)

# remove  words  that  occur  fewer  than  20  times  or in  fewer  than  2  documents
dfm_trimmed <- dfm_trim(dfm, min_termfreq=20, min_docfreq=2)
head(dfm_trimmed)
# total num of features after processing
nfeat(dfm_trimmed)
# total num of documents 
ndoc(dfm_trimmed)


dfm_trimmed_2 <- dfm_trim(dfm_2, min_termfreq = 20, min_docfreq = 2)
head(dfm_trimmed_2)
nfeat(dfm_trimmed_2)
ndoc(dfm_trimmed_2)
save.image("./Workspaces/dfm_2_new.RData")
```

## LDA 

### LDA Topic Modeling
```{r lda, echo=TRUE}
lda_tm <- LDA(dfm_trimmed, k = 25, method = "Gibbs", iter=3000, control = list(seed = 1234))
lda_tm
save.image("./Workspaces/LDA_topicmodel.RData")

log_likelihood_lda <- lda_tm@loglikelihood
log_likelihood_lda

# get top 10 words that contribute to each topic 
top10_words_lda <- get_terms(lda_tm, k=10)
top10_words_lda

# save the  top  10  words  over  all  25  topics,  for  later  use.
save(top10_words_lda, file = "./Data/top10words_lda.Rdata")

save.image("./Workspaces/LDA_topicmodel_top10words.RData")

# find most likely topic for each country
most_likely_topics_lda <- topics(lda_tm)
most_likely_topics_lda

lda_topics <- lda_tm@gamma
lda_topics <- t(lda_topics)

most_prob_topic_lda <- apply(lda_topics, 2, which.max)
table_prob_topics_lda <- table(most_prob_topic_lda)
table_prob_topics_lda

# sort the table
top_transcript_topics_lda <- sort(table(most_prob_topic_lda), decreasing=TRUE)
top_transcript_topics_lda
top_terms_lda_20 <- get_terms(lda_tm, k=20) 
top_terms_lda_20 

write.csv(top_terms_lda_20,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/top_terms_lda_20.csv")

save.image("./Workspaces/LDA_topicmodel_master.RData")

top_terms_lda_50 <- get_terms(lda_tm, k=50) 
top_terms_lda_50 
write.csv(top_terms_lda_50,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/top_terms_lda_50.csv")


```

### DFM 2 - LDA
```{r lda2, echo = TRUE}
# lda on DFM 2
lda_tm_2 <- LDA(dfm_trimmed_2, k = 25, method = "Gibbs", iter=3000, control = list(seed = 1234))
lda_tm_2

save.image("./Workspaces/LDA_topicmodel_2.RData")

log_likelihood_lda_2 <- lda_tm_2@loglikelihood
log_likelihood_lda_2

# get top 10 words that contribute to each topic 
top10_words_lda_2_2 <- get_terms(lda_tm_2, k=10)
top10_words_lda_2_2

# save the  top  10  words  over  all  25  topics,  for  later  use.
save(top10_words_lda_2_2, file = "./Data/top10words_lda_2.Rdata")

save.image("./Workspaces/LDA_topicmodel_top10words_2.RData")

# find most likely topic for each country
most_likely_topics_lda_2 <- topics(lda_tm_2)
most_likely_topics_lda_2

lda_topics_2 <- lda_tm_2@gamma
lda_topics_2 <- t(lda_topics_2)

most_prob_topic_lda_2 <- apply(lda_topics_2, 2, which.max)
table_prob_topics_lda_2 <- table(most_prob_topic_lda_2)
table_prob_topics_lda_2

# sort the table
top_transcript_topics_lda_2 <- sort(table(most_prob_topic_lda_2), decreasing=TRUE)
top_transcript_topics_lda_2
top_terms_lda_20_2 <- get_terms(lda_tm_2, k=20) 
top_terms_lda_20_2 

write.csv(top_terms_lda_20_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/top_terms_lda_20_2.csv")

save.image("./Workspaces/LDA_topicmodel_master_2.RData")

top_terms_lda_50_2 <- get_terms(lda_tm_2, k=50) 
top_terms_lda_50_2
write.csv(top_terms_lda_50_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/top_terms_lda_50_2.csv")


```

### Examine top contribuuting topics
```{r top_lda, echo = True}

top_topics_lda <- data.frame(title = covid_corp$documents$title, country = covid_corp$documents$country, top_topic = most_prob_topic_lda, date = covid_corp$documents$date)
top_topics_lda

save(top_topics_lda, file = "./Data/top_topics_lda.Rdata")
save.image("./Workspaces/top_lda_topics.RData")

write.csv(top_topics_lda,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/lda_topics.csv")

# for dfm 2
top_topics_lda_2 <- data.frame(title = covid_corp$documents$title, country = covid_corp$documents$country, top_topic = most_prob_topic_lda_2, date = covid_corp$documents$date)
top_topics_lda_2

save(top_topics_lda_2, file = "./Data/top_topics_lda_2.Rdata")
save.image("./Workspaces/top_lda_topics_2.RData")

write.csv(top_topics_lda_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/lda_topics_2.csv")

```




```{r more_lda, echo = TRUE}
load("./Workspaces/top_lda_topics.RData")

# find this for each country
US <- top_topics_lda[top_topics_lda$country == "US" ,]
UK <- top_topics_lda[top_topics_lda$country == "UK" ,]
AU <- top_topics_lda[top_topics_lda$country == "AU" ,]
NZ <- top_topics_lda[top_topics_lda$country == "NZ" ,]
CA <- top_topics_lda[top_topics_lda$country == "CA" ,]
unclassified <- top_topics_lda[top_topics_lda$country == "country" ,]

# order by date column
US$date <- as.numeric(US$date)
UK$date <- as.numeric(UK$date)
AU$date <- as.numeric(AU$date)
NZ$date <- as.numeric(NZ$date)
CA$date <- as.numeric(CA$date)

# plot the terms for only the top 1 topic
# US_plot
US_plot <- ggplot(US, aes(x=date, y=top_topic, pch="Top Topic")) 
US_plot +theme_bw() + ylab("Topic Number") + ggtitle("US Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# UK_plot
UK_plot <- ggplot(UK, aes(x=date, y=top_topic, pch="Top Topic")) 
UK_plot +theme_bw() + ylab("Topic Number") + ggtitle("UK Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# AU_plot
AU_plot <- ggplot(AU, aes(x=date, y=top_topic, pch="Top Topic")) 
AU_plot +theme_bw() + ylab("Topic Number") + ggtitle("AU Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# NZ_plot
NZ_plot <- ggplot(NZ, aes(x=date, y=top_topic, pch="Top Topic")) 
NZ_plot +theme_bw() + ylab("Topic Number") + ggtitle("NZ Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# US_plot
CA_plot <- ggplot(CA, aes(x=date, y=top_topic, pch="Top Topic")) 
CA_plot +theme_bw() + ylab("Topic Number") + ggtitle("CA Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

save.image("./Workspaces/top_lda_topic_plots.RData")

```

### Lda for DFM 2
```{r echo = TRUE}

load("./Workspaces/top_lda_topics_2.RData")

# find this for each country
US <- top_topics_lda_2[top_topics_lda_2$country == "US" ,]
UK <- top_topics_lda_2[top_topics_lda_2$country == "UK" ,]
AU <- top_topics_lda_2[top_topics_lda_2$country == "AU" ,]
NZ <- top_topics_lda_2[top_topics_lda_2$country == "NZ" ,]
CA <- top_topics_lda_2[top_topics_lda_2$country == "CA" ,]
unclassified <- top_topics_lda_2[top_topics_lda_2$country == "country" ,]

# order by date column
US$date <- as.numeric(US$date)
UK$date <- as.numeric(UK$date)
AU$date <- as.numeric(AU$date)
NZ$date <- as.numeric(NZ$date)
CA$date <- as.numeric(CA$date)

# plot the terms for only the top 1 topic
# US_plot
US_plot_2 <- ggplot(US, aes(x=date, y=top_topic, pch="Top Topic")) 
US_plot +theme_bw() + ylab("Topic Number") + ggtitle("US Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# UK_plot
UK_plot <- ggplot(UK, aes(x=date, y=top_topic, pch="Top Topic")) 
UK_plot +theme_bw() + ylab("Topic Number") + ggtitle("UK Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# AU_plot
AU_plot <- ggplot(AU, aes(x=date, y=top_topic, pch="Top Topic")) 
AU_plot +theme_bw() + ylab("Topic Number") + ggtitle("AU Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# NZ_plot
NZ_plot <- ggplot(NZ, aes(x=date, y=top_topic, pch="Top Topic")) 
NZ_plot +theme_bw() + ylab("Topic Number") + ggtitle("NZ Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# US_plot
CA_plot <- ggplot(CA, aes(x=date, y=top_topic, pch="Top Topic")) 
CA_plot +theme_bw() + ylab("Topic Number") + ggtitle("CA Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

save.image("./Workspaces/top_lda_topic_plots_2.RData")




```


### Find the contribution of each LDA topic to each country's top topics
```{r topic_contrib, echo = TRUE}
topics_df_lda <- data.frame(t(data.frame(lda_topics)))
head(topics_df_lda)

save(topics_df_lda, file = "./Data/top_topics_lda_df.Rdata")
save.image("./Workspaces/top_lda_topics_df.RData")
load("./Workspaces/top_lda_topics_df.RData")

write.csv(topics_df_lda,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/topics_df_lda.csv")

names(topics_df_lda) <- seq(1:ncol(topics_df_lda))
topics_df_lda$country <- top_topics_lda$country
topics_df_lda

write.csv(topics_df_lda,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/topics_contribution_lda.csv")

most_likely_topics_lda
# 11, 5, 23, 11, 8, 23, 5, 11, 8, 23, 3
top_5_topics_lda <- aggregate(cbind(topics_df_lda$'11', topics_df_lda$'5', topics_df_lda$'23', topics_df_lda$'8', topics_df_lda$'3') ~ country, data=topics_df_lda, FUN=mean)
top_5_topics_lda

save.image("./Workspaces/top_5_lda_topics.RData")

load("./Workspaces/top_5_lda_topics.RData")

# add topic names from earlier 
names(top_5_topics_lda) <- c('Country', '11: President / People / Want / Thank', '5: French-Canadian People / Help / Foreign / Support', '23: National Health Service / Hospital / Health', '8: COVID-19 Cases', '3: Work / Import / Nation / Measures')
top_5_topics_lda

```

### Trying this for DFM 2 
```{r echo = TRUE}
topics_df_lda_2 <- data.frame(t(data.frame(lda_topics_2)))
head(topics_df_lda_2)

save(topics_df_lda_2, file = "./Data/top_topics_lda_df_2.Rdata")
save.image("./Workspaces/top_lda_topics_df_2.RData")
load("./Workspaces/top_lda_topics_df_2.RData")

write.csv(topics_df_lda_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/topics_df_lda_2.csv")


names(topics_df_lda_2) <- seq(1:ncol(topics_df_lda_2))
topics_df_lda_2$country <- top_topics_lda_2$country
topics_df_lda_2

write.csv(topics_df_lda_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/topics_contribution_lda_2.csv")
most_likely_topics_lda_2
# 13, 15, 25, 2,24, 5, 15, 13, 24, 12, 14
top_5_topics_lda_2 <- aggregate(cbind(topics_df_lda_2$'13', topics_df_lda_2$'15', topics_df_lda_2$'25', topics_df_lda_2$'2', topics_df_lda_2$'24') ~ country, data=topics_df_lda_2, FUN=mean)
top_5_topics_lda_2

save.image("./Workspaces/top_5_lda_topics_2.RData")

load("./Workspaces/top_5_lda_topics_2.RData")

top_terms_lda_50_2
# add topic names from earlier 
names(top_5_topics_lda_2) <- c('Country', '13: The People', '15: French-Canadian People', '25: Social Distancing', '2: Reopening', '24: COVID-19 Cases')
top_5_topics_lda_2





```



Topic Stability 
```{r topic_stability, echo = TRUE}
# dfm trimmed is the original quanteda dfm without removing numbers 
lda_tm_2 <- LDA(dfm_trimmed, k = 25, method = "Gibbs", iter=3000, control = list(seed = 12345))

save.image("./Workspaces/top_5_lda_topics_2.RData")
load("./Workspaces/top_5_lda_topics_2.RData")

log_likelihood_2 <- lda_tm_2@loglikelihood
log_likelihood_2

similarity_between_lda_models <- simil(lda_tm@beta, lda_tm_2@beta, method = 'cosine')
head(similarity_between_lda_models)

# topic that is the closest match in the original run in terms of cosine similarity of the topic distribution over words
match_max <- apply(similarity_between_lda_models,1, which.max)
match_topics <- as.data.frame(cbind(seq(1:nrow(similarity_between_lda_models)), match_max))
names(match_topics)[1] <- "LDA Topic Model 1"
names(match_topics)[2] <- "LDA Topic Model 2"

match_topics

# num of words in top 10 words shared by each matched topic pair
as.data.frame(top10_words_lda)

# k = 10 for the top 10 words shared by each mateched topic pair
top10_words_lda_2 <- get_terms(lda_tm_2, k=10)
as.data.frame(top10_words_lda_2)

# shared words
# begin a counter
shared_words_num=0
# for loop for all the rows in match_topics table above
for (i in 1:nrow(match_topics)) {
   topic_number = as.numeric(match_topics$`LDA Topic Model 1`[i])
   top10_words_new_1 = top10_words_lda[, topic_number]
   matched_topic_number = as.numeric(match_topics$`LDA Topic Model 2`[i])
   top10_words_new_2 = top10_words_lda_2[, matched_topic_number]
   shared_words_num[i] = length(intersect(top10_words_new_1, top10_words_new_2))
}

match_topics$shared_words_num <- shared_words_num
match_topics$avg_shared_words <- shared_words_num/10
match_topics
```

## Topic Stability for DFM 2 
```{r echo = TRUE}

# dfm trimmed is the newl quanteda dfm without numbers 
lda_tm_2_2 <- LDA(dfm_trimmed_2, k = 25, method = "Gibbs", iter=3000, control = list(seed = 12345))

save.image("./Workspaces/top_5_lda_topics_2_2.RData")
load("./Workspaces/top_5_lda_topics_2_2.RData")

log_likelihood_2_2 <- lda_tm_2_2@loglikelihood
log_likelihood_2_2

similarity_between_lda_models_2 <- simil(lda_tm_2@beta, lda_tm_2_2@beta, method = 'cosine')
head(similarity_between_lda_models_2)

# topic that is the closest match in the original run in terms of cosine similarity of the topic distribution over words
match_max_2 <- apply(similarity_between_lda_models_2,1, which.max)
match_topics_2 <- as.data.frame(cbind(seq(1:nrow(similarity_between_lda_models_2)), match_max_2))
names(match_topics_2)[1] <- "LDA Topic Model 1, DFM 2"
names(match_topics_2)[2] <- "LDA Topic Model 2, DFM 2"

match_topics_2

# num of words in top 10 words shared by each matched topic pair
as.data.frame(top10_words_lda_2_2)

# k = 10 for the top 10 words shared by each mateched topic pair
top10_words_lda_2_2_2 <- get_terms(lda_tm_2_2, k=10)
as.data.frame(top10_words_lda_2_2_2)

# shared words
# begin a counter
shared_words_num_2=0
# for loop for all the rows in match_topics table above
for (i in 1:nrow(match_topics_2)) {
   topic_number = as.numeric(match_topics_2$`LDA Topic Model 1, DFM 2`[i])
   top10_words_new_1 = top10_words_lda_2_2[, topic_number]
   matched_topic_number = as.numeric(match_topics$`LDA Topic Model 2, DFM 2`[i])
   top10_words_new_2 = top10_words_lda_2_2_2[, matched_topic_number]
   shared_words_num[i] = length(intersect(top10_words_new_1, top10_words_new_2))
}

match_topics_2$shared_words_num <- shared_words_num_2
match_topics_2$avg_shared_words <- shared_words_num_2/10
match_topics_2

save.image("./Workspaces/match_topics_DFM2.RData")
load("./Workspaces/match_topics_DFM2.RData")
```


## Topic Models with Covariates

The Structural Topic Model (STM) is designed to incorporate document-level variables into a standard topic model. Since, we have information about both the country and the date of the transcripts, we can use an STM (from the stm package) to model the eﬀects of these covariates directly.
```{r echo = TRUE}
# incorporate date
# date <- as.numeric(covid_corp$documents$date)
# NAs introduced by coercion --> not sure what this means
# https://stackoverflow.com/questions/8215404/change-from-date-and-hour-format-to-numeric-format

date <- as.POSIXct(covid_corp$documents$date, format="%b %d, %Y")
date <- as.numeric(date)
date

```
Fit an STM model where the topic content varies according to this binary variable, and where the prevalence varies according to both this binary variable and the spline of the date variable you’ve created. Be sure to use the spectral initialization and set k=0, which will allow the STM function to automatically select a number of topics using the spec- tral learning method. Keep in mind that this function is computationally demanding, so start with the minimum threshold document frequency threshold set to 10; if your computer takes an unreasonably long time to fit the STM model with this threshold, you can raise it to as high as 30. Report the number of topics selected in the fitted model. Also report the number of iterations completed before the model converged.

## STM 

Identify and name each of the 5 topics that occur in the highest proportion of documents using the following code: plot(fit.stm, type = “summary”)
```{r echo = TRUE}
# convert our dfm into a dataframe
dfm_2

df_2_stm <- as.data.frame(cbind(covid_corp$documents$texts, covid_corp$documents$date, covid_corp$documents$country))
df_2_stm


save.image("./Workspaces/topic_models_w_covariates.RData")
load("./Workspaces/topic_models_w_covariates.RData")

# set the columns
names(df_2_stm) <- c("texts", "date", "country")
df_2_stm$country <- as.factor(df_2_stm$country)
df_2_stm$date <- date
df_2_stm$texts <- as.character(df_2_stm$texts)
df_2_stm

save.image("./Workspaces/stm_dfm.RData")
load("./Workspaces/stm_dfm.RData")


system.time(
transcript_stm <- stm(dfm_2, K=0, init.type='Spectral', seed=12345, prevalence =~country + s(date), content =~country, data=dfm_2_stm))

save.image("./Workspaces/stm_dfm_model.RData")
load("./Workspaces/stm_dfm_model.RData")
transcript_stm 

stm_plot <- plot(transcript_stm, type='summary')
```


Using the visualization commands in the stm package, discuss one of these top 5 topics. How does the content vary with the country discussing that topic? How does the prevalence change over time?

```{r echo = TRUE}
save.image("./Workspaces/stm_plot.RData")
load("./Workspaces/stm_plot.RData")

country <- estimateEffect(c(56) ~ country, transcript_stm, meta=df_2_stm)

# need to pick 2 countries here     
au_ca_plot <- plot(transcript_stm, type = "perspectives", topics = c(56))
au_ca_plot 
# plot(country, "country", model = transcript_stm,
#      method = "difference", cov.value1 = "US", cov.value2 = "UK", cov.value3 = "CA", cov.value4 = "NZ", cov.value5 = "AU", cov.value6 = "other")

# plot(country, "country", model = transcript_stm,
#      method = "difference", cov.value1 = "US", cov.value2 = "other")

# How does the prevalence change over time?
time <- estimateEffect(c(51) ~ date, transcript_stm, meta=df_2_stm)

plot(time, "date", transcript_stm, method="continuous", xaxt = "n", xlab="Date")
```

## Wordfish (Non-Parametric Scaling)

### New DFM for US and NZ (subset)
```{r echo = TRUE}
# pick a subset with only NZ and US

subset_nz_us_corpus <- corpus_subset(covid_corp, country %in% c('US', 'NZ'))

save.image("./Workspaces/wordfish.RData")
load("./Workspaces/wordfish.RData")

nz_us_dfm <- dfm(subset_nz_us_corpus, tolower=TRUE, stem = TRUE,  remove = c(stopwords("english"), tad_stopwords), remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_hyphens = TRUE, remove_symbols = TRUE)
nz_us_dfm

save.image("./Workspaces/nz_us_dfm.RData")
load("./Workspaces/nz_us_dfm.RData")

```

Quanteda’s implementation of Wordfish, textmodel wordfish, requires that we provide the indices for a pair of documents to globally identify θ, the latent dimension of interest. In this case, we are looking to estimate the latent covid ideological dimension. Use the indices of the 1979 Labor and Conservative manifestos to do so. That is, set dir = c(index of 1979 Labor manifesto, index of 1979 Conservative manifesto).

```{r echo = TRUE}

corpus_index <- data.frame(subset_nz_us_corpus$documents)
corpus_index

# Date 13 US transcript
idx_13_us <- which(corpus_index$country == "US" & corpus_index$date == "Apr 14, 2020")
idx_13_us
# 1979 Conservative manifestos
idx_13_nz <- which(corpus_index$country == "NZ" & corpus_index$date == "Apr 14, 2020")
idx_13_nz 

# dir = c(index of 1979 Labor manifesto, index of 1979 Conservative manifesto)
dir  = c(idx_13_us,  idx_13_nz)
dir

us_nz_wordfish <- textmodel_wordfish(nz_us_dfm, dir)
us_nz_wordfish

summary(us_nz_wordfish)

save.image("./Workspaces/wf_model.RData")
load("./Workspaces/wf_model.RData")


# turn wordfish model into a df to find the max theta
us_nz_wordfish_df <- data.frame(cbind(us_nz_wordfish$theta, corpus_index$country, corpus_index$date))
us_nz_wordfish_df

# not sure what a high theta means?? discuss this later
most_leftwing_transcript <- us_nz_wordfish_df[which.max(us_nz_wordfish_df$X1),]
most_leftwing_transcript

most_rightwing_transcript <- us_nz_wordfish_df[which.min(us_nz_wordfish_df$X1),]
most_rightwing_transcript


save.image("./Workspaces/wf_model_theta.RData")
load("./Workspaces/wf_model_theta.RData")
```

## Burstiness
```{r echo = TRUE}
# use the same dfm 2 

bursty <- function(word, DTM, date) {
  word.vec <- DTM[, which(colnames(DTM) == word)]
  if(length(word.vec) == 0) {
    print(paste(word, " does not exist in this corpus."))
    return()
  } 
  else {
    word.times <- c(0,which(as.vector(word.vec)>0))
    
    kl <- kleinberg(word.times, gamma = 0.5)
    kl$start <- date[kl$start+1]
    kl$end <- date[kl$end]
    max_level <- max(kl$level)
    
    plot(c(kl$start[1], kl$end[1]), c(1,max_level),
         type = "n", xlab = "Time", ylab = "Level", bty = "n",
         xlim = c(min(date), max(date)), ylim = c(1, max_level),
         yaxt = "n")
    axis(2, at = 1:max_level)
    
    for (i in 1:nrow(kl)) {
      if (kl$start[i] != kl$end[i]) {
        arrows(kl$start[i], kl$level[i], kl$end[i], kl$level[i], code = 3, angle = 90,
               length = 0.05)
      } 
      else {
        points(kl$start[i], kl$level[i])
      }
    }
    
    print(kl)
  }
  #note deviation from standard defaults bec don't have that much data
}
docvars(covid_corp)$num_date <- as.numeric(as.POSIXct(covid_corp$documents$date, format="%b %d, %Y"))
# docvars(covid_corp)$date <- as.numeric(gsub("[[:alpha:]]","", docvars(covid_corp)$date))


save.image("./Workspaces/burstiness.RData")
load("./Workspaces/burstiness.RData")

# 3.1 Evaluating the burstiness of several key words
bursty("covid", dfm_2, docvars(covid_corp)$num_date)

# need the years here
bursty("coronavirus", dfm_2, docvars(covid_corp)$num_date)

# need the years here
bursty("case", dfm_2, docvars(covid_corp)$num_date)

bursty("health", dfm_2, docvars(covid_corp)$num_date)

save.image("./Workspaces/burstiness.RData")
load("./Workspaces/burstiness.RData")
```

## Dimension Reduction and Semantics 

### subset to US News (new dfm)
```{r echo = TRUE}

# subset the corpus to only pick category in world news
us_corpus <- corpus_subset(covid_corp, country %in% c('US'))
us_corpus 

us_dfm <- dfm(us_corpus, tolower=TRUE, stem = TRUE,  remove = c(stopwords("english"), tad_stopwords), remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_hyphens = TRUE, remove_symbols = TRUE)
us_dfm

# create a matrix
us_matrix <- convert(us_dfm, to = "matrix") 
```

### PCA
```{r echo = TRUE}
# get PCA
us_pca <- prcomp(us_matrix, center = TRUE, scale = TRUE)
# worldnews_pca
# sanity check
us_pca$rotation[1:10, 1:5]

save.image("./Workspaces/pca.RData")
load("./Workspaces/pca.RData")
```


### top 5 positive and negative words in US subset
```{r echo = TRUE}
# rank according to loadings
us_pcomp1 <- sort(us_pca$rotation[,1], decreasing = T)
head(us_pcomp1)

us_pccomp1_increasing <- sort(us_pca$rotation[,1])
head(us_pccomp1_increasing)


top5_positive_words_us <- us_pcomp1[1:5]
top5_positive_words_us


top5_negative_words_us <- us_pccomp1_increasing[1:5]
top5_negative_words_us

save.image("./Workspaces/top5_pos_words.RData")
load("./Workspaces/top5_pos_words.RData")
```

## LSA
```{r echo = TRUE}
# convert dfm to lsa
us_matrix_lsa <- convert(us_dfm, to = "lsa") 

us_matrix_lsa <- lw_logtf(us_matrix_lsa) * gw_idf(us_matrix_lsa)
head(us_matrix_lsa, 10)


save.image("./Workspaces/lsa.RData")
load("./Workspaces/lsa.RData")


# Find close terms in a textmatrix 
associate(us_matrix_lsa, "coronavirus", "cosine", threshold = .2)[1:5]
associate(us_matrix_lsa, "coronavirus", "cosine", threshold = .2)[1:10]

# Find close terms in a textmatrix 
associate(us_matrix_lsa, "case", "cosine", threshold = .2)[1:5]

# Find close terms in a textmatrix 
associate(us_matrix_lsa, "health", "cosine", threshold = .2)[1:5]

save.image("./Workspaces/lsa.RData")
load("./Workspaces/lsa.RData")

```



## Webscraping 
R Webscraping Code 
```{r scrape_text, echo = TRUE}

# 
# #Parse JSON of urls of all covid related transcripts
# json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
# 
# df <- lapply(json_data, function(url) 
# {
#   data.frame(matrix(unlist(url), ncol = 2, byrow=T))
# })
# 
# df <- do.call(rbind, df)
# 
# colnames(df) <- c('url', 'title')
# rownames(df) <- NULL
# 
# #Correcting URLs
# url_prefix <-'https://www.rev.com/blog/transcripts/'
# titles <- gsub(' ', '-', df$title) %>% tolower()
# titles <- iconv(titles, 'utf-8', 'ascii', sub='')
# titles <- gsub(':', '', titles) 
# df$url <- Map(paste, url_prefix, titles, sep = "")
# 
# #Selecting only transcripts of Country Leaders
# toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task', 'U.S.', 'Jacinda', 'Ardern')
# matches <- df[(grepl(paste(toMatch,collapse='|'), 
#                      df$title)), ]
# row.names(matches) <- NULL
# 
# 
# #Special URLs
# matches$url[19] = 'https://www.rev.com/blog/transcripts/donald-trump-coronavirus-press-briefing-transcript-april-16'
# matches$url[69] = 'https://www.rev.com/blog/transcripts/transcript-trump-signs-historic-2-trillion-coronavirus-stimulus-bill'
# matches$url[93] = 'https://www.rev.com/blog/transcripts/justin-trudeau-coronavirus-update-for-canada-march-20'
# matches$url[110] = 'https://www.rev.com/blog/transcripts/donald-trump-mike-pence-and-coronavirus-update-transcript-march-9'
# 
# 
# #Function to scrape text without speaker names
# scrapetextonly <- function(url, title){
#   transcript <- read_html(url)
#   text <- transcript %>% 
#     rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>% 
#     xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>% 
#     rvest::html_text()
#   
#   text_fname = paste(title, '.txt', sep = '')
#   text_fname = paste('Data/all_texts/', text_fname, sep = '')
#   write_file(text_fname, text)
#   return(toString(text))
# }
# 
# write_file <- function(fname, data){
#   fileConn<-file(fname)
#   writeLines(data, fileConn)
#   close(fileConn)  
# }
# 
# 
# #try catch for scraping
# try_scrape <- function(url, title) {
#   out <- tryCatch(
#     {
#       scrapetextonly(url, title)
#     },
#     error=function(cond) {
#       message(paste("URL does not seem to exist:", url))
#       message("Here's the original error message:")
#       message(cond)
#       return(NA)
#     }
#   )    
#   return(out)
# }
# 
# 
# #Scraping the text
# errs <- c()
# texts <- c()
# for( i in 1:nrow(matches)){
#   trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
#   if(is.na(trans)){
#     errs <- append(errs, i)
#   }
#   texts <- append(texts, trans)
# }
# 
# #Adding texts as column to dataframe
# matches$text <- texts[2:121]
# 
# 
# save.image('scraped.RData')
# load('scraped.RData')

```

## Workspace Saving
Saving... 
```{r echo=TRUE}
# save workspace
save.image("./Workspaces/workspace_master.RData")
rm(list = ls())
load("./Workspaces/workspace_master.RData")

```




